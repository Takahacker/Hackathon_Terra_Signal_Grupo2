{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8d99abd8-86e4-4307-8316-c109fae1b1ec",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Terra Signal Hackathon\n",
    "This notebook is provided as a starting point. Feel free to use it, discard it, modify it, or pretend it doesn't exist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "14c40c2a-4dc2-42db-b4b1-92190de5a3f7",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Read file with pandas"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Read the CSV file using pandas\n",
    "file_path = \"./history.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "df.head().transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "52c25b6e-625e-41a5-8a8e-1b7ead6f3db7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d515decb-86f9-4525-ba88-a4aafbcf0e95",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Limpeza dos dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d35b7b0d-749c-4640-ab1f-6dd10a749bcc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pdf[\"tokens\"] = pdf[\"CustomerFeedback_clean\"].str.split()\n",
    "\n",
    "# 0 = não churn (positivo), 1 = churn (negativo)\n",
    "pdf_pos = pdf[pdf[\"Churn\"] == 0]\n",
    "pdf_neg = pdf[pdf[\"Churn\"] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7efad4a9-c8b3-4ed1-bc56-695c3f8f1829",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "pos_counts = Counter(\n",
    "    w\n",
    "    for tokens in pdf_pos[\"tokens\"]\n",
    "    for w in tokens\n",
    ")\n",
    "\n",
    "neg_counts = Counter(\n",
    "    w\n",
    "    for tokens in pdf_neg[\"tokens\"]\n",
    "    for w in tokens\n",
    ")\n",
    "\n",
    "len(pos_counts), len(neg_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0fc816b8-2259-4d0a-ab8e-e9d8e8f7b600",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "all_words = set(pos_counts) | set(neg_counts)\n",
    "\n",
    "total_pos = sum(pos_counts.values())\n",
    "total_neg = sum(neg_counts.values())\n",
    "V = len(all_words)  # vocabulário para suavização\n",
    "\n",
    "rows = []\n",
    "for w in all_words:\n",
    "    pos = pos_counts.get(w, 0)\n",
    "    neg = neg_counts.get(w, 0)\n",
    "    total = pos + neg\n",
    "\n",
    "    # filtra palavras muito raras (ajuste o limiar)\n",
    "    if total < 5:\n",
    "        continue\n",
    "\n",
    "    # probabilidades suavizadas\n",
    "    p_pos = (pos + 1) / (total_pos + V)\n",
    "    p_neg = (neg + 1) / (total_neg + V)\n",
    "\n",
    "    log_odds = np.log(p_neg / p_pos)\n",
    "\n",
    "    rows.append((w, pos, neg, total, log_odds))\n",
    "\n",
    "word_stats = pd.DataFrame(\n",
    "    rows,\n",
    "    columns=[\"word\", \"pos_count\", \"neg_count\", \"total\", \"log_odds\"]\n",
    ")\n",
    "\n",
    "word_stats.sort_values(\"log_odds\", ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ceb03927-3a8f-4ca8-b39a-83e532050707",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b8a757d7-a0f3-45b8-a113-960c8a44513e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# principais negativas\n",
    "neg_words = (\n",
    "    word_stats.sort_values(\"log_odds\", ascending=False)\n",
    "              .head(100)\n",
    ")\n",
    "\n",
    "# principais positivas\n",
    "pos_words = (\n",
    "    word_stats.sort_values(\"log_odds\", ascending=True)\n",
    "              .head(100)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b7e63675-7eb5-46bc-a46a-11c5a34175b6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# dicionários palavra -> peso (uso total ou |log_odds|)\n",
    "pos_freq = {row.word: row.total for _, row in pos_words.iterrows()}\n",
    "neg_freq = {row.word: row.total for _, row in neg_words.iterrows()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "57d0d8f4-74e3-41f4-82d9-9f3a8933be4f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "wc_pos = WordCloud(\n",
    "    width=800,\n",
    "    height=400,\n",
    "    background_color=\"white\"\n",
    ").generate_from_frequencies(pos_freq)\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.imshow(wc_pos, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Palavras mais associadas a NÃO churn (positivas)\")\n",
    "display(plt.gcf())\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cdee0789-585f-49c3-922e-cf9d9b48684f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "wc_neg = WordCloud(\n",
    "    width=800,\n",
    "    height=400,\n",
    "    background_color=\"white\"\n",
    ").generate_from_frequencies(neg_freq)\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.imshow(wc_neg, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Palavras mais associadas a churn (negativas)\")\n",
    "display(plt.gcf())\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9cecabd2-7c78-46e4-86c4-5b7bb37372af",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark_word_stats = spark.createDataFrame(word_stats)\n",
    "spark_word_stats.write.mode(\"overwrite\").saveAsTable(\"workspace.churn.word_polarity\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2a4ff5af-c805-40a3-846e-ca127030ffe6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install scikit-learn wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0d4c5560-a11a-4a9e-9c03-a48c1b709d84",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# Naive Bayes de palavras (positivo x negativo) + WordCloud\n",
    "# Baseado na ideia do projeto Spark, mas em Python puro (sem Spark)\n",
    "# Usa a coluna \"CustomerFeedback_clean\" e \"Churn\" de um CSV\n",
    "#  - Churn = 0  -> feedback positivo (não churn)\n",
    "#  - Churn = 1  -> feedback negativo (churn)\n",
    "# ============================================\n",
    "\n",
    "# Instale as dependências (rode UMA vez no ambiente):\n",
    "# pip install pandas numpy wordcloud matplotlib\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from math import log10\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ------------------------------------------------\n",
    "# 1. Carregar dados\n",
    "# ------------------------------------------------\n",
    "# Ajuste o caminho do arquivo aqui:\n",
    "CSV_PATH = \"history_clean.csv\"\n",
    "\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "\n",
    "# Garante que as colunas necessárias existem\n",
    "col_label = \"Churn\"\n",
    "col_text  = \"CustomerFeedback_clean\"\n",
    "\n",
    "if col_label not in df.columns or col_text not in df.columns:\n",
    "    raise ValueError(f\"O CSV precisa ter as colunas '{col_label}' e '{col_text}'.\")\n",
    "\n",
    "# Remove linhas sem texto\n",
    "df = df.dropna(subset=[col_text])\n",
    "\n",
    "# Garante tipos corretos\n",
    "df[col_label] = df[col_label].astype(int)\n",
    "df[col_text]  = df[col_text].astype(str)\n",
    "\n",
    "# ------------------------------------------------\n",
    "# 2. Construir contagens de palavras por classe (0 = não churn, 1 = churn)\n",
    "# ------------------------------------------------\n",
    "pos_counter = Counter()  # não churn (positivo)\n",
    "neg_counter = Counter()  # churn (negativo)\n",
    "\n",
    "for _, row in df.iterrows():\n",
    "    label = row[col_label]\n",
    "    text  = row[col_text].lower()\n",
    "    tokens = text.split()\n",
    "    if label == 0:\n",
    "        for w in tokens:\n",
    "            pos_counter[w] += 1\n",
    "    elif label == 1:\n",
    "        for w in tokens:\n",
    "            neg_counter[w] += 1\n",
    "    # Se tiver outros valores de Churn, são ignorados\n",
    "\n",
    "# ------------------------------------------------\n",
    "# 3. Calcular log P(palavra | classe) com Laplace\n",
    "# ------------------------------------------------\n",
    "vocab = set(pos_counter.keys()) | set(neg_counter.keys())\n",
    "V = len(vocab)\n",
    "\n",
    "total_pos = sum(pos_counter.values())\n",
    "total_neg = sum(neg_counter.values())\n",
    "\n",
    "alpha = 1  # Laplace smoothing\n",
    "\n",
    "# modelo: word -> (log_prob_pos, log_prob_neg)\n",
    "modelo = {}\n",
    "\n",
    "for w in vocab:\n",
    "    c_pos = pos_counter.get(w, 0)\n",
    "    c_neg = neg_counter.get(w, 0)\n",
    "\n",
    "    # P(w | classe) com Laplace\n",
    "    p_pos = (c_pos + alpha) / (total_pos + alpha * V)\n",
    "    p_neg = (c_neg + alpha) / (total_neg + alpha * V)\n",
    "\n",
    "    log_p_pos = log10(p_pos)\n",
    "    log_p_neg = log10(p_neg)\n",
    "\n",
    "    modelo[w] = (log_p_pos, log_p_neg)\n",
    "\n",
    "# ------------------------------------------------\n",
    "# 4. Função classificador (opcional, caso queira classificar textos)\n",
    "# ------------------------------------------------\n",
    "def classificar_texto(texto):\n",
    "    \"\"\"\n",
    "    Retorna 0 (não churn / positivo) ou 1 (churn / negativo)\n",
    "    com base no modelo Naive Bayes de palavras.\n",
    "    \"\"\"\n",
    "    texto = str(texto).lower()\n",
    "    tokens = texto.split()\n",
    "\n",
    "    score_pos = 0.0\n",
    "    score_neg = 0.0\n",
    "\n",
    "    for p in tokens:\n",
    "        if p in modelo:\n",
    "            log_p_pos, log_p_neg = modelo[p]\n",
    "            score_pos += log_p_pos\n",
    "            score_neg += log_p_neg\n",
    "\n",
    "    # Se a soma de log-probs for maior em churn, retorna 1 (negativo)\n",
    "    return 1 if score_neg > score_pos else 0\n",
    "\n",
    "# Exemplo de uso:\n",
    "# print(classificar_texto(\"service is terrible and very slow\"))\n",
    "# print(classificar_texto(\"great support, very happy\"))\n",
    "\n",
    "# ------------------------------------------------\n",
    "# 5. Medir polaridade de cada palavra (positivo x negativo)\n",
    "#    score = log P(w | churn=1) - log P(w | churn=0)\n",
    "#    score > 0 -> palavra mais associada a churn (negativa)\n",
    "#    score < 0 -> palavra mais associada a não churn (positiva)\n",
    "# ------------------------------------------------\n",
    "palavras = []\n",
    "log_pos_list = []\n",
    "log_neg_list = []\n",
    "score_list   = []\n",
    "count_list   = []\n",
    "\n",
    "for w, (log_p_pos, log_p_neg) in modelo.items():\n",
    "    score = log_p_neg - log_p_pos  # alinhado com \"negativo\" = churn\n",
    "    total_count = pos_counter.get(w, 0) + neg_counter.get(w, 0)\n",
    "    palavras.append(w)\n",
    "    log_pos_list.append(log_p_pos)\n",
    "    log_neg_list.append(log_p_neg)\n",
    "    score_list.append(score)\n",
    "    count_list.append(total_count)\n",
    "\n",
    "word_stats = pd.DataFrame({\n",
    "    \"word\": palavras,\n",
    "    \"log_prob_pos\": log_pos_list,\n",
    "    \"log_prob_neg\": log_neg_list,\n",
    "    \"score\": score_list,\n",
    "    \"count\": count_list\n",
    "})\n",
    "\n",
    "# Opcional: filtrar palavras muito raras\n",
    "min_count = 5\n",
    "word_stats = word_stats[word_stats[\"count\"] >= min_count].reset_index(drop=True)\n",
    "\n",
    "# ------------------------------------------------\n",
    "# 6. Selecionar top palavras positivas e negativas\n",
    "# ------------------------------------------------\n",
    "TOP_N = 100\n",
    "\n",
    "# Negativas: mais associadas a churn (score alto)\n",
    "neg_words = word_stats.sort_values(\"score\", ascending=False).head(TOP_N)\n",
    "\n",
    "# Positivas: mais associadas a não churn (score bem negativo)\n",
    "pos_words = word_stats.sort_values(\"score\", ascending=True).head(TOP_N)\n",
    "\n",
    "print(\"Top 10 palavras NEGATIVAS (churn):\")\n",
    "print(neg_words[[\"word\", \"score\", \"count\"]].head(10))\n",
    "print(\"\\nTop 10 palavras POSITIVAS (não churn):\")\n",
    "print(pos_words[[\"word\", \"score\", \"count\"]].head(10))\n",
    "\n",
    "# ------------------------------------------------\n",
    "# 7. WordCloud das principais palavras negativas e positivas\n",
    "#    Usamos |score| como peso (quanto mais forte a polaridade,\n",
    "#    maior a palavra na nuvem)\n",
    "# ------------------------------------------------\n",
    "neg_freq = {row.word: abs(row.score) for _, row in neg_words.iterrows()}\n",
    "pos_freq = {row.word: abs(row.score) for _, row in pos_words.iterrows()}\n",
    "\n",
    "# Nuvem de palavras NEGATIVAS (churn) – em VERMELHO\n",
    "wc_neg = WordCloud(\n",
    "    width=800,\n",
    "    height=400,\n",
    "    background_color=\"white\",\n",
    "    colormap=\"Reds\"  # <- paleta vermelha\n",
    ").generate_from_frequencies(neg_freq)\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.imshow(wc_neg, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Palavras mais associadas a churn (negativas)\")\n",
    "plt.show()\n",
    "\n",
    "# Nuvem de palavras POSITIVAS (não churn) – em VERDE\n",
    "wc_pos = WordCloud(\n",
    "    width=800,\n",
    "    height=400,\n",
    "    background_color=\"white\",\n",
    "    colormap=\"Greens\"  # <- paleta verde\n",
    ").generate_from_frequencies(pos_freq)\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.imshow(wc_pos, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Palavras mais associadas a NÃO churn (positivas)\")\n",
    "plt.show()\n",
    "\n",
    "# ------------------------------------------------\n",
    "# 8. (Opcional) Salvar a tabela de polaridade em CSV\n",
    "# ------------------------------------------------\n",
    "# word_stats.to_csv(\"word_polarity_naive_bayes.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "02a391fe-600d-4830-b287-b86aa0877347",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ------------------------------------------------\n",
    "# 4.1. Avaliar acurácia do modelo Naive Bayes\n",
    "# ------------------------------------------------\n",
    "\n",
    "# Cria uma coluna com a previsão do modelo para cada feedback\n",
    "df[\"pred_nb\"] = df[col_text].apply(classificar_texto)\n",
    "\n",
    "# Acurácia global\n",
    "accuracy = (df[\"pred_nb\"] == df[col_label]).mean()\n",
    "print(f\"Acurácia global do Naive Bayes: {accuracy:.4f}\")\n",
    "\n",
    "# Matriz de confusão\n",
    "confusion = pd.crosstab(df[col_label], df[\"pred_nb\"],\n",
    "                        rownames=[\"Churn real\"],\n",
    "                        colnames=[\"Churn previsto\"])\n",
    "print(\"\\nMatriz de confusão:\")\n",
    "print(confusion)\n",
    "\n",
    "# Acurácia separada por classe (opcional)\n",
    "acc_por_classe = (\n",
    "    df.assign(acerto=(df[\"pred_nb\"] == df[col_label]).astype(int))\n",
    "      .groupby(col_label)[\"acerto\"]\n",
    "      .mean()\n",
    ")\n",
    "print(\"\\nAcurácia por classe (0 = não churn, 1 = churn):\")\n",
    "print(acc_por_classe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a890d873-3d4e-4ff7-bcf2-acbe49f2a598",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# Naive Bayes de palavras (positivo x negativo) + WordCloud\n",
    "# Agora com split TREINO / TESTE e avaliação de acurácia em TESTE\n",
    "#\n",
    "# Usa a coluna \"CustomerFeedback_clean\" e \"Churn\" de um CSV\n",
    "#  - Churn = 0  -> feedback positivo (não churn)\n",
    "#  - Churn = 1  -> feedback negativo (churn)\n",
    "# ============================================\n",
    "\n",
    "# Dependências (rode UMA vez):\n",
    "# pip install pandas numpy wordcloud matplotlib scikit-learn\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from math import log10\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# ------------------------------------------------\n",
    "# 1. Carregar dados\n",
    "# ------------------------------------------------\n",
    "CSV_PATH = \"history_clean.csv\"\n",
    "\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "\n",
    "col_label = \"Churn\"\n",
    "col_text  = \"CustomerFeedback_clean\"\n",
    "\n",
    "if col_label not in df.columns or col_text not in df.columns:\n",
    "    raise ValueError(f\"O CSV precisa ter as colunas '{col_label}' e '{col_text}'.\")\n",
    "\n",
    "df = df.dropna(subset=[col_text])\n",
    "\n",
    "df[col_label] = df[col_label].astype(int)\n",
    "df[col_text]  = df[col_text].astype(str)\n",
    "\n",
    "# ------------------------------------------------\n",
    "# 1.1 Split TREINO / TESTE\n",
    "# ------------------------------------------------\n",
    "df_train, df_test = train_test_split(\n",
    "    df,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=df[col_label]  # mantém proporção de classes\n",
    ")\n",
    "\n",
    "print(f\"Tamanho treino: {len(df_train)} | Tamanho teste: {len(df_test)}\")\n",
    "\n",
    "# ------------------------------------------------\n",
    "# 2. Construir contagens de palavras por classe (apenas TREINO)\n",
    "# ------------------------------------------------\n",
    "pos_counter = Counter()  # não churn (positivo)\n",
    "neg_counter = Counter()  # churn (negativo)\n",
    "\n",
    "for _, row in df_train.iterrows():\n",
    "    label = row[col_label]\n",
    "    text  = row[col_text].lower()\n",
    "    tokens = text.split()\n",
    "    if label == 0:\n",
    "        for w in tokens:\n",
    "            pos_counter[w] += 1\n",
    "    elif label == 1:\n",
    "        for w in tokens:\n",
    "            neg_counter[w] += 1\n",
    "\n",
    "# ------------------------------------------------\n",
    "# 3. Calcular log P(palavra | classe) com Laplace (modelo treinado)\n",
    "# ------------------------------------------------\n",
    "vocab = set(pos_counter.keys()) | set(neg_counter.keys())\n",
    "V = len(vocab)\n",
    "\n",
    "total_pos = sum(pos_counter.values())\n",
    "total_neg = sum(neg_counter.values())\n",
    "\n",
    "alpha = 1  # Laplace smoothing\n",
    "\n",
    "# modelo: word -> (log_prob_pos, log_prob_neg)\n",
    "modelo = {}\n",
    "\n",
    "for w in vocab:\n",
    "    c_pos = pos_counter.get(w, 0)\n",
    "    c_neg = neg_counter.get(w, 0)\n",
    "\n",
    "    p_pos = (c_pos + alpha) / (total_pos + alpha * V)\n",
    "    p_neg = (c_neg + alpha) / (total_neg + alpha * V)\n",
    "\n",
    "    log_p_pos = log10(p_pos)\n",
    "    log_p_neg = log10(p_neg)\n",
    "\n",
    "    modelo[w] = (log_p_pos, log_p_neg)\n",
    "\n",
    "# ------------------------------------------------\n",
    "# 4. Função classificador\n",
    "# ------------------------------------------------\n",
    "def classificar_texto(texto):\n",
    "    \"\"\"\n",
    "    Retorna 0 (não churn / positivo) ou 1 (churn / negativo)\n",
    "    com base no modelo Naive Bayes de palavras.\n",
    "    \"\"\"\n",
    "    texto = str(texto).lower()\n",
    "    tokens = texto.split()\n",
    "\n",
    "    score_pos = 0.0\n",
    "    score_neg = 0.0\n",
    "\n",
    "    for p in tokens:\n",
    "        if p in modelo:\n",
    "            log_p_pos, log_p_neg = modelo[p]\n",
    "            score_pos += log_p_pos\n",
    "            score_neg += log_p_neg\n",
    "\n",
    "    return 1 if score_neg > score_pos else 0\n",
    "\n",
    "# ------------------------------------------------\n",
    "# 4.1 Avaliar acurácia em TESTE\n",
    "# ------------------------------------------------\n",
    "df_test = df_test.copy()\n",
    "df_test[\"pred_nb\"] = df_test[col_text].apply(classificar_texto)\n",
    "\n",
    "accuracy_test = (df_test[\"pred_nb\"] == df_test[col_label]).mean()\n",
    "print(f\"\\nAcurácia em TESTE do Naive Bayes: {accuracy_test:.4f}\")\n",
    "\n",
    "confusion_test = pd.crosstab(\n",
    "    df_test[col_label],\n",
    "    df_test[\"pred_nb\"],\n",
    "    rownames=[\"Churn real\"],\n",
    "    colnames=[\"Churn previsto\"]\n",
    ")\n",
    "print(\"\\nMatriz de confusão (TESTE):\")\n",
    "print(confusion_test)\n",
    "\n",
    "acc_por_classe = (\n",
    "    df_test.assign(acerto=(df_test[\"pred_nb\"] == df_test[col_label]).astype(int))\n",
    "           .groupby(col_label)[\"acerto\"]\n",
    "           .mean()\n",
    ")\n",
    "print(\"\\nAcurácia por classe (TESTE) (0 = não churn, 1 = churn):\")\n",
    "print(acc_por_classe)\n",
    "\n",
    "# ------------------------------------------------\n",
    "# 5. Medir polaridade de cada palavra (usando o modelo treinado)\n",
    "# ------------------------------------------------\n",
    "palavras = []\n",
    "log_pos_list = []\n",
    "log_neg_list = []\n",
    "score_list   = []\n",
    "count_list   = []\n",
    "\n",
    "for w, (log_p_pos, log_p_neg) in modelo.items():\n",
    "    score = log_p_neg - log_p_pos  # >0: mais associada a churn\n",
    "    total_count = pos_counter.get(w, 0) + neg_counter.get(w, 0)\n",
    "    palavras.append(w)\n",
    "    log_pos_list.append(log_p_pos)\n",
    "    log_neg_list.append(log_p_neg)\n",
    "    score_list.append(score)\n",
    "    count_list.append(total_count)\n",
    "\n",
    "word_stats = pd.DataFrame({\n",
    "    \"word\": palavras,\n",
    "    \"log_prob_pos\": log_pos_list,\n",
    "    \"log_prob_neg\": log_neg_list,\n",
    "    \"score\": score_list,\n",
    "    \"count\": count_list\n",
    "})\n",
    "\n",
    "# Opcional: filtrar palavras muito raras\n",
    "min_count = 5\n",
    "word_stats = word_stats[word_stats[\"count\"] >= min_count].reset_index(drop=True)\n",
    "\n",
    "# ------------------------------------------------\n",
    "# 6. Selecionar top palavras positivas e negativas\n",
    "# ------------------------------------------------\n",
    "TOP_N = 100\n",
    "\n",
    "neg_words = word_stats.sort_values(\"score\", ascending=False).head(TOP_N)\n",
    "pos_words = word_stats.sort_values(\"score\", ascending=True).head(TOP_N)\n",
    "\n",
    "print(\"\\nTop 10 palavras NEGATIVAS (churn):\")\n",
    "print(neg_words[[\"word\", \"score\", \"count\"]].head(10))\n",
    "print(\"\\nTop 10 palavras POSITIVAS (não churn):\")\n",
    "print(pos_words[[\"word\", \"score\", \"count\"]].head(10))\n",
    "\n",
    "# ------------------------------------------------\n",
    "# 7. WordCloud das principais palavras negativas e positivas\n",
    "# ------------------------------------------------\n",
    "neg_freq = {row.word: abs(row.score) for _, row in neg_words.iterrows()}\n",
    "pos_freq = {row.word: abs(row.score) for _, row in pos_words.iterrows()}\n",
    "\n",
    "wc_neg = WordCloud(\n",
    "    width=800,\n",
    "    height=400,\n",
    "    background_color=\"white\",\n",
    "    colormap=\"Reds\"\n",
    ").generate_from_frequencies(neg_freq)\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.imshow(wc_neg, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Palavras mais associadas a churn (negativas)\")\n",
    "plt.show()\n",
    "\n",
    "wc_pos = WordCloud(\n",
    "    width=800,\n",
    "    height=400,\n",
    "    background_color=\"white\",\n",
    "    colormap=\"Greens\"\n",
    ").generate_from_frequencies(pos_freq)\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.imshow(wc_pos, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Palavras mais associadas a NÃO churn (positivas)\")\n",
    "plt.show()\n",
    "\n",
    "# ------------------------------------------------\n",
    "# 8. (Opcional) Salvar a tabela de polaridade em CSV\n",
    "# ------------------------------------------------\n",
    "# word_stats.to_csv(\"word_polarity_naive_bayes.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a905853c-4044-4e08-a766-b67e063a1e82",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ------------------------------------------------\n",
    "# Avaliar acurácia em TESTE\n",
    "# ------------------------------------------------\n",
    "df_test = df_test.copy()\n",
    "df_test[\"pred_nb\"] = df_test[col_text].apply(classificar_texto)\n",
    "\n",
    "# Acurácia global\n",
    "accuracy_test = (df_test[\"pred_nb\"] == df_test[col_label]).mean()\n",
    "print(f\"Acurácia em TESTE do Naive Bayes: {accuracy_test:.4f}\")\n",
    "\n",
    "# Matriz de confusão\n",
    "confusion_test = pd.crosstab(\n",
    "    df_test[col_label],\n",
    "    df_test[\"pred_nb\"],\n",
    "    rownames=[\"Churn real\"],\n",
    "    colnames=[\"Churn previsto\"]\n",
    ")\n",
    "print(\"\\nMatriz de confusão (TESTE):\")\n",
    "print(confusion_test)\n",
    "\n",
    "# Acurácia por classe\n",
    "acc_por_classe = (\n",
    "    df_test.assign(acerto=(df_test[\"pred_nb\"] == df_test[col_label]).astype(int))\n",
    "           .groupby(col_label)[\"acerto\"]\n",
    "           .mean()\n",
    ")\n",
    "print(\"\\nAcurácia por classe (TESTE) (0 = não churn, 1 = churn):\")\n",
    "print(acc_por_classe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c01f0fb1-f895-4963-901b-c633e951e644",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Quantos rótulos diferentes cada texto pode ter?\n",
    "df = your_dataframe_hereambig = (\n",
    "\n",
    "    df.groupby(\"CustomerFeedback_clean\")[\"Churn\"]\n",
    "      .nunique()\n",
    "      .value_counts()\n",
    ")\n",
    "\n",
    "print(ambig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0f4af054-26bd-4b37-b028-647b9116f448",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "06a1c5bd-dfdb-472f-96d3-ff0c491ebd84",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# =====================================================\n",
    "# Naive Bayes Correto para Feedback de Clientes (Churn)\n",
    "# - Usa treino/teste para evitar leakage\n",
    "# - Calcula priors corretamente\n",
    "# - Gera probabilidades, não só labels\n",
    "# - Pode ser usado como meta-feature no modelo final\n",
    "# =====================================================\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from math import log10\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# -----------------------------------------------------\n",
    "# 1. Carregar dataset e preparar dados\n",
    "# -----------------------------------------------------\n",
    "df = pd.read_csv(\"history_clean.csv\")\n",
    "\n",
    "TEXT_COL = \"CustomerFeedback_clean\"\n",
    "TARGET_COL = \"Churn\"\n",
    "\n",
    "df = df.dropna(subset=[TEXT_COL])\n",
    "df[TEXT_COL] = df[TEXT_COL].astype(str)\n",
    "df[TARGET_COL] = df[TARGET_COL].astype(int)\n",
    "\n",
    "# -----------------------------------------------------\n",
    "# 2. Separar treino e teste (ESSENCIAL para evitar leakage!)\n",
    "# -----------------------------------------------------\n",
    "train_df, test_df = train_test_split(\n",
    "    df, \n",
    "    test_size=0.2, \n",
    "    stratify=df[TARGET_COL], \n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(\"Treino:\", len(train_df), \" - Teste:\", len(test_df))\n",
    "\n",
    "# -----------------------------------------------------\n",
    "# 3. Função que treina Naive Bayes com bag-of-words\n",
    "# -----------------------------------------------------\n",
    "def treinar_naive_bayes(df_train, text_col, label_col, alpha=1):\n",
    "\n",
    "    pos_counter = Counter()\n",
    "    neg_counter = Counter()\n",
    "\n",
    "    total_pos_docs = 0\n",
    "    total_neg_docs = 0\n",
    "\n",
    "    # Tokenizar e contar palavras por classe\n",
    "    for _, row in df_train.iterrows():\n",
    "        label = row[label_col]\n",
    "        tokens = row[text_col].lower().split()\n",
    "\n",
    "        if label == 0:\n",
    "            total_pos_docs += 1\n",
    "            for w in tokens:\n",
    "                pos_counter[w] += 1\n",
    "        else:\n",
    "            total_neg_docs += 1\n",
    "            for w in tokens:\n",
    "                neg_counter[w] += 1\n",
    "\n",
    "    vocab = set(pos_counter.keys()) | set(neg_counter.keys())\n",
    "    V = len(vocab)\n",
    "\n",
    "    total_pos_words = sum(pos_counter.values())\n",
    "    total_neg_words = sum(neg_counter.values())\n",
    "\n",
    "    # Calcular priors corretos\n",
    "    total_docs = total_pos_docs + total_neg_docs\n",
    "    log_prior_pos = log10(total_pos_docs / total_docs)\n",
    "    log_prior_neg = log10(total_neg_docs / total_docs)\n",
    "\n",
    "    modelo = {\n",
    "        \"vocab\": vocab,\n",
    "        \"pos_counter\": pos_counter,\n",
    "        \"neg_counter\": neg_counter,\n",
    "        \"total_pos_words\": total_pos_words,\n",
    "        \"total_neg_words\": total_neg_words,\n",
    "        \"log_prior_pos\": log_prior_pos,\n",
    "        \"log_prior_neg\": log_prior_neg,\n",
    "        \"V\": V,\n",
    "        \"alpha\": alpha\n",
    "    }\n",
    "\n",
    "    return modelo\n",
    "\n",
    "# -----------------------------------------------------\n",
    "# 4. Treinar o Naive Bayes no TREINO apenas\n",
    "# -----------------------------------------------------\n",
    "nb_model = treinar_naive_bayes(train_df, TEXT_COL, TARGET_COL)\n",
    "print(\"Vocabulário:\", nb_model[\"V\"], \"palavras\")\n",
    "\n",
    "# -----------------------------------------------------\n",
    "# 5. Função de previsão (log-probabilidade + probabilidade)\n",
    "# -----------------------------------------------------\n",
    "def nb_pred_prob(texto, model):\n",
    "    texto = str(texto).lower()\n",
    "    tokens = texto.split()\n",
    "\n",
    "    score_pos = model[\"log_prior_pos\"]\n",
    "    score_neg = model[\"log_prior_neg\"]\n",
    "\n",
    "    alpha = model[\"alpha\"]\n",
    "    V = model[\"V\"]\n",
    "\n",
    "    for w in tokens:\n",
    "        # P(w|pos)\n",
    "        c_pos = model[\"pos_counter\"].get(w, 0)\n",
    "        p_pos = (c_pos + alpha) / (model[\"total_pos_words\"] + alpha * V)\n",
    "        score_pos += log10(p_pos)\n",
    "\n",
    "        # P(w|neg)\n",
    "        c_neg = model[\"neg_counter\"].get(w, 0)\n",
    "        p_neg = (c_neg + alpha) / (model[\"total_neg_words\"] + alpha * V)\n",
    "        score_neg += log10(p_neg)\n",
    "\n",
    "    # Converter scores logarítmicos em probabilidade real\n",
    "    # usando logística (sigmoid)\n",
    "    diff = score_neg - score_pos\n",
    "    prob = 1 / (1 + 10**(-diff))\n",
    "\n",
    "    return prob  # probabilidade de churn\n",
    "\n",
    "def nb_pred_label(texto, model):\n",
    "    prob = nb_pred_prob(texto, model)\n",
    "    return 1 if prob >= 0.5 else 0\n",
    "\n",
    "# -----------------------------------------------------\n",
    "# 6. Validar no conjunto de teste (SEM LEAKAGE)\n",
    "# -----------------------------------------------------\n",
    "test_df[\"nb_prob\"] = test_df[TEXT_COL].apply(lambda x: nb_pred_prob(x, nb_model))\n",
    "test_df[\"nb_pred\"] = (test_df[\"nb_prob\"] >= 0.5).astype(int)\n",
    "\n",
    "from sklearn.metrics import classification_report, roc_auc_score\n",
    "\n",
    "print(\"\\n=== DESEMPENHO DO NAIVE BAYES NO TESTE ===\")\n",
    "print(classification_report(test_df[TARGET_COL], test_df[\"nb_pred\"]))\n",
    "print(\"ROC-AUC:\", roc_auc_score(test_df[TARGET_COL], test_df[\"nb_prob\"]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7849ff28-3663-4b6f-ac10-0ce95f2c08d4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 8217042190192805,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Gustavo",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
